{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKyDF/bfVJFPYhFO9Jrf/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajivbits/IPython-Notebooks/blob/master/PerigonAI_Evaluation_Rajiv_C_(Data_Scientist).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem 1: Python programming, data processing.\n",
        "\n",
        "\n",
        "\n",
        "In this problem we want to generate pseudo-random data that has certain desired statistical properties. This can be useful for demo, research or testing purposes.\n",
        "\n",
        "First, let’s generate these “desired statistical properties”.\n",
        "\n",
        " - Generate a random 6x6 correlation matrix rho.\n",
        "\n",
        " - **Regularization**: write a test checking that rho is a valid correlation matrix, and if not - find the nearest valid one.\n",
        "\n",
        "Now, let’s generate the data that would have these properties.\n",
        "\n",
        " - Generate a set of 6 random variables (put them in a matrix 1000x6, the distribution doesn’t matter, but should be continuous), distributed between 0 and 1 with correlation defined by rho."
      ],
      "metadata": {
        "id": "aPMWD8jRti6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below Python code performs the following tasks:\n",
        "    \n",
        "    1) Generates a random 6x6 matrix using numpy.\n",
        "    2) Creates a correlation matrix from the random matrix using the np.corrcoef function.\n",
        "    3) Defines a function is_pos_def to check if a matrix is positive definite.\n",
        "    4) Checks if the correlation matrix is valid using the is_pos_def function.\n",
        "    5) If the correlation matrix is not valid, it finds the nearest valid one by adding a small value to the diagonal and then prints the nearest valid correlation matrix. Otherwise, it prints the generated matrix as a valid correlation matrix.\n",
        "    \n",
        "    The code utilizes NumPy for matrix operations and correlation coefficient calculation.\n",
        "    It also demonstrates the use of the np.eye function to create an identity matrix and the\n",
        "    np.linalg.eigvals function to check for positive definiteness of the matrix."
      ],
      "metadata": {
        "id": "H7yvTg04t_Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate a random 6x6 matrix\n",
        "random_matrix = np.random.rand(6, 6)\n",
        "\n",
        "# Create a correlation matrix from the random matrix\n",
        "rho = np.corrcoef(random_matrix, rowvar=False)\n",
        "\n",
        "# Check if the correlation matrix is valid\n",
        "def is_pos_def(matrix):\n",
        "    return np.all(np.linalg.eigvals(matrix) > 0)\n",
        "\n",
        "# If the correlation matrix is not valid, find the nearest valid one\n",
        "if not is_pos_def(rho):\n",
        "    nearest_corr_matrix = np.corrcoef(random_matrix + np.eye(6) * 0.01, rowvar=False)\n",
        "    print(\"The nearest valid correlation matrix is: \\n\", nearest_corr_matrix)\n",
        "else:\n",
        "    print(\"The generated matrix is a valid correlation matrix: \\n\", rho)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knTGowBSuC9x",
        "outputId": "ebd18ee7-9ab7-4061-adba-ae6752ac57ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated matrix is a valid correlation matrix: \n",
            " [[ 1.         -0.05938082  0.09941322 -0.14773491 -0.7872452   0.41425299]\n",
            " [-0.05938082  1.          0.33169573  0.70918342  0.24296406  0.6341125 ]\n",
            " [ 0.09941322  0.33169573  1.          0.85562388  0.42443298 -0.09257481]\n",
            " [-0.14773491  0.70918342  0.85562388  1.          0.51485773  0.20146345]\n",
            " [-0.7872452   0.24296406  0.42443298  0.51485773  1.         -0.43635627]\n",
            " [ 0.41425299  0.6341125  -0.09257481  0.20146345 -0.43635627  1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J3yUlhvluhUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Slightly different, but related problem.\n",
        "\n",
        " - Apply PCA to reduce the dimensionality to 5.\n",
        " - Let the output variable y = round(x6).\n",
        " - Build a couple of classifiers of your choice to predict y from {x1, x2, …, x5}.\n",
        " - Compare their performance."
      ],
      "metadata": {
        "id": "Ub6tNEkzvWgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Generate a random 6x6 matrix\n",
        "random_matrix = np.random.rand(6, 6)\n",
        "\n",
        "# Apply PCA to reduce the dimensionality to 5\n",
        "pca = PCA(n_components=5)\n",
        "pca_matrix = pca.fit_transform(random_matrix)\n",
        "\n",
        "# Create a new variable y by rounding the 6th column of the random matrix\n",
        "y = np.round(random_matrix[:, 5])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(pca_matrix, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "classifiers = [\n",
        "    ('Linear Regression', LinearRegression()),\n",
        "    ('AdaBoost', AdaBoostRegressor()),\n",
        "    ('Random Forest', RandomForestRegressor()),\n",
        "    ('Support Vector Machine', NuSVR(kernel='rbf'))\n",
        "]\n",
        "\n",
        "for name, clf in classifiers:\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} accuracy: {r2_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbuEy2WSvk7_",
        "outputId": "f571c9eb-4f35-47d1-841d-c187687dc8a3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression accuracy: -3.1312149669052722\n",
            "AdaBoost accuracy: -1.0\n",
            "Random Forest accuracy: 0.22619999999999996\n",
            "Support Vector Machine accuracy: 0.22497953924701997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Data Science, Model Build\n",
        "\n",
        "**Dataset used** - 10_01_train_dataset.csv\n",
        "\n",
        "**Key Assumptions**\n",
        "\n",
        "Categorical variables are identified as object type variables and numberical variables which have less than 6 unique values\n",
        "\n",
        "**Dataset Description**\n",
        "\n",
        "You have been provided with a dataset that has 116 Rows, 123 Columns (mix of continuous and categorical variables)\n",
        "and a target column.\n",
        "\n",
        "The goal is to build a model that generalizes well over this dataset, you are free to transform the dataset as you\n",
        "feel necessary. We are not looking for the highest scoring model. Our goal is to understand your thought process and\n",
        "decision making.    \n",
        "    "
      ],
      "metadata": {
        "id": "6m02FStIwzvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('10_01_train_dataset.csv')\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "## Logarithmically transforming the target due to high variance\n",
        "y = np.log(df['target'])\n",
        "\n",
        "# Data Exploration and Preprocessing\n",
        "# Check for missing values\n",
        "print(X.isnull().sum())\n",
        "\n",
        "\n",
        "# Select continuous variables with distinct values greater than 10\n",
        "num_cols = []\n",
        "cat_cols = []\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['int64', 'float64']:  # Check if the column is numeric\n",
        "        if df[col].nunique() > 6:  # Check if the number of distinct values is greater than 6\n",
        "            num_cols.append(col)\n",
        "        else:\n",
        "            cat_cols.append(col)\n",
        "\n",
        "\n",
        "# Encode categorical variables\n",
        "\n",
        "categorical_cols = list(X.select_dtypes(include=['object']).columns.values)\n",
        "\n",
        "discrete_cols = categorical_cols + cat_cols\n",
        "\n",
        "# print(discrete_cols)\n",
        "\n",
        "\n",
        "for col in discrete_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Scale the continuous variables\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Feature Engineering\n",
        "# Perform dimensionality reduction using PCA\n",
        "pca = PCA(n_components=10)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Perform feature selection using chi-squared test\n",
        "# kbest = SelectKBest(score_func=chi2, k=10)\n",
        "# X_kbest = kbest.fit_transform(X, y)\n",
        "\n",
        "# Model Building\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Build and evaluate a baseline model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "print(f\"Linear Regression accuracy: {r2_score(y_test, y_pred)}\")\n",
        "\n",
        "# Experiment with different algorithms\n",
        "classifiers = [\n",
        "    ('AdaBoost', AdaBoostRegressor()),\n",
        "    ('Random Forest', RandomForestRegressor()),\n",
        "    ('Support Vector Machine', NuSVR(kernel='rbf'))\n",
        "]\n",
        "\n",
        "for name, clf in classifiers:\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} accuracy: {r2_score(y_test, y_pred)}\")\n",
        "    # print(f\"{name} classification report: \\n{classification_report(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "l0EzKy9rwajf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('10_01_train_dataset.csv')\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "## Logarithmically transforming the target due to high variance\n",
        "y = np.log(df['target'])\n",
        "\n",
        "# Data Exploration and Preprocessing\n",
        "# Check for missing values\n",
        "# print(X.isnull().sum())\n",
        "\n",
        "# Encode categorical variables\n",
        "num_cols = []\n",
        "cat_cols = []\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['int64', 'float64']:  # Check if the column is numeric\n",
        "        if df[col].nunique() > 6:  # Check if the number of distinct values is greater than 6\n",
        "            num_cols.append(col)\n",
        "        else:\n",
        "            cat_cols.append(col)\n",
        "\n",
        "\n",
        "# Encode categorical variables\n",
        "\n",
        "categorical_cols = list(X.select_dtypes(include=['object']).columns.values)\n",
        "\n",
        "discrete_cols = categorical_cols + cat_cols\n",
        "\n",
        "\n",
        "\n",
        "for col in discrete_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Scale the continuous variables\n",
        "scaler = MinMaxScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Feature Engineering\n",
        "# Perform dimensionality reduction using PCA\n",
        "pca = PCA(n_components=10)\n",
        "X_pca = pca.fit_transform(X[num_cols])\n",
        "\n",
        "# Perform feature selection using Lasso method\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X, y)\n",
        "lasso_coef = lasso.coef_\n",
        "lasso_cols = X.columns[lasso_coef != 0] #['fast_food_restaurant_0_25', 'restaurant_0_25']\n",
        "\n",
        "\n",
        "# Model Building\n",
        "# Split the data into training and testing sets, using lasso columns and discrete columns\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[num_cols + discrete_cols], y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Build and evaluate a baseline model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "print(f\"Linear Regression accuracy: {r2_score(y_test, y_pred)}\")\n",
        "\n",
        "# Experiment with different algorithms\n",
        "classifiers = [\n",
        "    ('Adaptive Boosting', AdaBoostRegressor()),\n",
        "    ('Random Forest', RandomForestRegressor()),\n",
        "    ('Support Vector Machine', NuSVR(kernel='rbf'))\n",
        "]\n",
        "\n",
        "for name, clf in classifiers:\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{name} accuracy: {r2_score(y_test, y_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P683R9JyjvN",
        "outputId": "ce1db0db-db26-4e4c-9eb5-d50deae07bfd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression accuracy: -20.17779049924262\n",
            "Adaptive Boosting accuracy: 0.08648863503492699\n",
            "Random Forest accuracy: 0.10479318626963885\n",
            "Support Vector Machine accuracy: 0.04379590760421792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3\n",
        "\n",
        "It's 3000 BC, and you are the leader of a tribe of 4000 people. You are leading your tribe to a new location where you must build a circular settlement from scratch. How big will it be and how long will it take to build a stone wall around it?"
      ],
      "metadata": {
        "id": "C_Hs0cKh-YCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, I am leading a tribe of 4000 people to a new location where I must build a circular settlement from scratch.\n",
        "\n",
        "To determine the size and construction time of the settlement, I would consider the following factors:\n",
        "\n",
        "**Population**\n",
        "\n",
        "The size of the settlement will be determined by the number of people in the tribe. Known variable (4,000)\n",
        "\n",
        "**Land area**\n",
        "\n",
        "The available land area will influence the size of the settlement. (Unknown)\n",
        "\n",
        "**Building materials**\n",
        "\n",
        "The availability and accessibility of building materials will affect the construction time and the size of the settlement.\n",
        "\n",
        "**Labor**\n",
        "\n",
        "The number of workers and their skills will impact the construction time.\n",
        "\n",
        "**Technology**\n",
        "\n",
        "The level of technology available for quarrying, transporting, and placing the stones will also affect the construction time.\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Without specific information about the available land area, building materials, labor force, and technology, it is difficult to provide an exact size and construction time for the settlement and constructing a stone wall.\n",
        "\n",
        "However, the factors mentioned above can be used to make informed decisions and plan the construction accordingly."
      ],
      "metadata": {
        "id": "NsYeJsaN-xqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4\n",
        "\n",
        "Is there an inconsistency in the following paragraph?:\n",
        "\n",
        "\"A suburban located Starbucks makes on average $100,000 per month in revenue and has 10,500 square meters of an adjacent area dedicated to parking for visitors only. Despite good revenue and overall satisfaction with service, both the staff and visitors are complaining that parking is full more than half of the time.\""
      ],
      "metadata": {
        "id": "6fd7NFwwABtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "This statement is Inconsistent, assuming that there is NO typographical errors in this statement while mentioning the units of measurement.\n",
        "\n",
        "It states that the Starbucks has 10,500 square meters of an adjacent area dedicated to parking for visitors only. This seems unusually large for a suburban location.\n",
        "\n",
        "A more typical size for a parking area at a suburban Starbucks might be around 10,500 square feet, rather than square meters."
      ],
      "metadata": {
        "id": "VRa0U_ghApyh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcFRUqkR-eKs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}