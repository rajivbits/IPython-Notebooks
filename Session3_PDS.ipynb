{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Data Science - Classroom to Careers\n",
    "\n",
    "### Session 3\n",
    "\n",
    "#### Topics to be covered\n",
    "\n",
    " - Eigenvalues and Eigenvectors ( pick up from where we left / questions)\n",
    " - Basic Calculus - Differentiation and Integration\n",
    " - Maximum Likelihood estimation\n",
    " - Parametric vs. Non-parametric estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "**How do we determine the Eigenvalues and Eigenvectors corresponding to a Matrix?**\n",
    "\n",
    "An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it. \n",
    "\n",
    "Consider the image below in which three vectors are shown. The green square is only drawn to illustrate the linear transformation that is applied to each of these three vectors.\n",
    "\n",
    "<img src = \"https://www.visiondummy.com/wp-content/uploads/2014/03/eigenvectors.png\">\n",
    "\n",
    "\n",
    "When a given **nxn** Matrix is given we will have to identify a vector **v** such that \n",
    "\n",
    "  **A**.**v**=λ.**v**  \n",
    "  \n",
    "  where - \n",
    "  \n",
    "  **v** is the Eigen Vector of Matrix A\n",
    "  \n",
    "  **λ** is the Eigen value corresponding to the eigen vector **v**\n",
    "\n",
    "***Understanding with an Example***\n",
    "\n",
    "Compute the Eigen-value(s) and Eigen-vector(s) corresponding to the Matrix A\n",
    "\n",
    "$$A = \\begin{bmatrix} 2 & 3 \\\\ 2 & 1 \\end{bmatrix}$$\n",
    "\n",
    "Compute the Eigencalues of matrix A\n",
    "\n",
    "\n",
    "\n",
    "$$\\text{Find a vector }\\vec{v}^{\\,} \\text{ such that }A\\vec{v}^{\\,} = λ\\vec{v}^{\\,}$$\n",
    "\n",
    "\n",
    "$$A\\vec{v}^{\\,} - λ\\vec{v}^{\\,} = 0$$\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} 2 & 3 \\\\ 2 & 1 \\end{bmatrix} - λ\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = 0$$\n",
    "\n",
    "$$\\begin{bmatrix} 2-λ & 3 \\\\ 2 & 1-λ \\end{bmatrix} - λ\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = 0$$\n",
    "\n",
    "\n",
    "image and content courtesy: www.visiondummy.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiation\n",
    "\n",
    "A differential of any function is the rate of change. \n",
    "\n",
    "Or in other words it is the slope of the tangent for a curve (a limiting point, defined only for contibuous functions)\n",
    "\n",
    "$$\\frac{d}{dx}f(x) = f'(x) = \\lim_{h\\to0}  \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "There are other ways of definining this - \n",
    "\n",
    " - Differential Coefficient\n",
    " - First Derivative\n",
    "\n",
    "**Applications of Derivatives**\n",
    "\n",
    "1) Simple application for estimation\n",
    "\n",
    "Let us understand the application of derivatives with a simple example\n",
    "\n",
    "Given a value find the value\n",
    "\n",
    "$$\\log_{10} 2 = 0.3010, \\\\ \\log_{2} 24 = ? $$ \n",
    "\n",
    "2) Estimate a change in function and find whether the function is increasing or decrasing\n",
    "\n",
    "$$ f(x) = x^{2} + 5x + 6 $$\n",
    "\n",
    "$$find {f'(x)} {and} {f''(x)}$$\n",
    "\n",
    "\n",
    "### Integration\n",
    "\n",
    "This is a reverse process of differentiation, it is also called piece wise summation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric vs. Non - parametric estimation\n",
    "\n",
    "**Parametric estimation**\n",
    "\n",
    "Algorithms that simplify the function to a known form are called parametric machine learning algorithms.\n",
    "\n",
    "The algorithms involve two steps:\n",
    "\n",
    " - Select a form for the function.\n",
    " - Learn the coefficients for the function from the training data.\n",
    " - An easy to understand functional form for the mapping function is a line, as is used in linear regression:\n",
    "\n",
    "**a0 + a1*x1 + a2*x2 + .... + an*xn = 0**\n",
    "\n",
    "Where a0, a1, a2...an are the coefficients of the line that control the intercept and slope, and x1,x2...xn are input variables.\n",
    "\n",
    "**Non-parametric estimation**\n",
    "\n",
    "Nonparametric methods seek to best fit the training data in constructing the mapping function, whilst maintaining some ability to generalize to unseen data. As such, they are able to fit a large number of functional forms.\n",
    "\n",
    "Benefits of Nonparametric Machine Learning Algorithms:\n",
    "\n",
    "**Flexibility**: Capable of fitting a large number of functional forms.\n",
    "**Power**: No assumptions (or weak assumptions) about the underlying function.\n",
    "**Performance**: Can result in higher performance models for prediction.\n",
    "\n",
    "**Limitations of Nonparametric Machine Learning Algorithms**\n",
    "\n",
    "**More data**: Require a lot more training data to estimate the mapping function.\n",
    "**Slower**: A lot slower to train as they often have far more parameters to train.\n",
    "**Overfitting**: More of a risk to overfit the training data and it is harder to explain why specific predictions are made.\n",
    "\n",
    "References - https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
